//  This progrma computes a simple version of matrix multiplication

#include<algorithm>
#include<cassert>
#include<cstdlib>
#include<functional>
#include<iostream>
#include<vector>

using std::cout;
using std::generate;
using std::vector;

// âœ… What is std?
// In C++, std is the standard namespace. It contains all the standard C++ library classes, functions, and objectsâ€”like cout, vector, generate, etc.

// To avoid writing std:: everywhere, we sometimes pull specific things into the current scope using using.

// ðŸ“Œ Breakdown of Each Line
// 1. using std::cout;
// Brings std::cout (the standard console output stream) into your current scope. This means instead of:


// std::cout << "Hello";
// you can just write:


// cout << "Hello";
// 2. using std::generate;
// Brings in the algorithm function std::generate, which fills a range (like a vector) with values generated by a function/lambda.

// Example:


// std::vector<int> v(10);
// std::generate(v.begin(), v.end(), [] { return rand() % 100; });
// This fills the vector with random values.

// 3. using std::vector;
// Brings the class template std::vector into scope. So instead of:


// std::vector<int> myVec;
// You can now write:

// vector<int> myVec;




__global__ void matrixMul(const int *a, const int *b, int *c, int N){
    //Compute each thread's global row and column index
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x *blockDim.x + threadIdx.x;

    // Iterate over row, and down column
    c[row * N + col] = 0;
    for(int k =0; k <N; k++){
        // Accumulate results for a single element
        c[row * N + col] += a[row * N + k] *b[k*N + col];
    }
}

void verify_result(vector<int> &a, vector<int> &b, vector<int> &c, int N) {
    // for every row ...
    for(int i=0; i <N; i++){
        // for every column...
        for(int j =0; j<N; j++){
            int tmp = 0;
            //  For every element in the row-column pair
            for (int k=0; k< N; k++){
                //  Accumulage the partial results
                tmp += a[i * N + k] * b[k * N + j];
            }

            //  Check against the CPU result
            assert(tmp == c[i * N +j]);
        }
    }

}


int main(){
    //  Matrix size of 1024 * 1024;
    // int N = 1 <<10;
    int N = 1 <<5; // To run on RTX 3070

    // Size (in bytes) of matrix
    size_t bytes = N * N * sizeof(int);

    //  Host vectors
    vector<int> h_a(N * N);
    vector<int> h_b(N * N);
    vector<int> h_c(N * N);


    //  Initialize matrices
    generate(h_a.begin(), h_a.end(), []() {return rand() % 100; });
    generate(h_b.begin(), h_b.end(), []() {return rand() % 100; });

    // Allocate device memory
    int *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, bytes);
    cudaMalloc(&d_b, bytes);
    cudaMalloc(&d_c, bytes);


    //  Copy data to the device
    cudaMemcpy(d_a, h_a.data(), bytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b.data(), bytes, cudaMemcpyHostToDevice);

    // Threads per CTA dimension
    int THREADS = 32;

    //  Blocks per grid dimension (assumes THREAD divides N evenly)
    int BLOCKS = N / THREADS;

    //  Use dim3 structs or block and grid dimensions
    dim3 threads(THREADS, THREADS);
    dim3 blocks(BLOCKS, BLOCKS);


    // Launch kernel
    matrixMul<<<blocks, threads>>>(d_a, d_b, d_c, N);

    //  Copy back to the host 
    cudaMemcpy(h_c.data(), d_c, bytes, cudaMemcpyDeviceToHost);

    //  Check result
    verify_result(h_a,h_b,h_c, N);

    cout<< "COMPLETED SUCCESSFULLY\n";


    //  Free memory on device
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    return 0;
}